# -*- coding: utf-8 -*-
"""Final_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZsCXscrGX0odCVn2DpuFkEqPKBpYHZly
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from imblearn.over_sampling import SMOTE

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve

from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

"""# Importing necessary libraries"""

!pip install pymongo

import pandas as pd
import pymongo
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from pymongo.errors import AutoReconnect

uri = "mongodb+srv://keerthanabellam2310:Keerthana23@cluster0.dpaomtn.mongodb.net/?retryWrites=true&w=majority&connectTimeoutMS=300000"
client = MongoClient(uri, server_api=ServerApi('1'))
df = pd.read_csv("diabetes_dataset.csv")
data = df.to_dict(orient='records')


db = client['BigDataProject']
collection = db['Diabetes_data']

try:
    batch_size = 1000
    for i in range(0, len(df), batch_size):
        batch = data[i : i + batch_size]
        collection.insert_many(batch, ordered=False)
except AutoReconnect as e:
    print(f"AutoReconnect error: {e}")

from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col, count
from pyspark.sql.types import IntegerType
import pymongo
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

spark = SparkSession.builder.appName("MongoDBDataCleaning").getOrCreate()

uri = "mongodb+srv://keerthanabellam2310:Keerthana23@cluster0.dpaomtn.mongodb.net/?retryWrites=true&w=majority&connectTimeoutMS=300000"
client = MongoClient(uri, server_api=ServerApi('1'))
db = client['BigDataProject']
collection = db['Diabetes_data']


# Read data from MongoDB into a PySpark DataFrame
diabetes_data = pd.DataFrame(list(collection.find()))
diabetes_data.info()

"""### Data Cleaning and Preprocessing"""

diabetes_data = diabetes_data.drop('_id', axis=1)

spark_df = spark.createDataFrame(diabetes_data)

spark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()

duplicate_rows = spark_df.count() - spark_df.dropDuplicates().count()
print(f"Number of duplicate rows: {duplicate_rows}")

if duplicate_rows > 0:
    spark_df = spark_df.dropDuplicates()
    print("Duplicate rows removed.")

spark_df.printSchema()

for column in ['Gender']:
    spark_df.groupBy(column).count().show()

spark_df.describe( ['age', 'bmi', 'location', 'year', 'gender']).show()

import matplotlib.pyplot as plt
import seaborn as sns

spark_df.select("age").rdd.flatMap(lambda x: x).histogram(10) # Adjust the number of bins as needed
plt.hist(spark_df.select("age").rdd.flatMap(lambda x: x).collect(), bins=10) # Adjust the number of bins as needed
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.title("Distribution of Age")
plt.show()

diabetes_cases_by_year = spark_df.groupBy("year").count()
print("\nCount of Diabetes Cases by Year:")
diabetes_cases_by_year.show()
display(diabetes_cases_by_year)

spark_df.columns

df=spark_df.toPandas()

df.head()

df.shape

df.info()

df.describe()

df.describe(include=['object'])

df.isnull().sum()

df.duplicated().sum()

df = df.drop_duplicates()

df.duplicated().sum()

df.shape

# Checking class imbalance
df['diabetes'].value_counts()

"""# Data Visualization"""

plt.figure(figsize=(6, 4))
sns.countplot(x='diabetes', data=df)
plt.title('Distribution of Diabetes Cases')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df['age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='gender', data=df)
plt.title('Gender Distribution')
plt.show()

df['gender'].value_counts()

df = pd.get_dummies(df, columns=['gender'], prefix=['gender'])

df.head()

df['smoking_history'].value_counts()

df = pd.get_dummies(df, columns=['smoking_history'], prefix=['smoking'])

df.head()

location_counts=df['location'].value_counts()
print(location_counts)

freq_encoding = location_counts.to_dict()
df['location_encoded'] = df['location'].map(freq_encoding)

df = df.drop('location', axis=1)

df.head()

corr = df.corr()
corr.to_excel("correlation_matrix.xlsx")

corr

plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""| Feature               | Importance Score (Max Correlation with Target 'diabetes') | Key Correlations                                              | Notes                                                       |
|-----------------------|----------------------------------------------------------|---------------------------------------------------------------|-------------------------------------------------------------|
| hbA1c_level           | 0.4007                                                   | Strongly correlated with diabetes (0.4007), blood_glucose_level (0.1667) | Critical predictor for diabetes.                             |
| blood_glucose_level   | 0.4196                                                   | Strongly correlated with diabetes (0.4196), hbA1c_level (0.1667) | Critical predictor for diabetes.                             |
| bmi                   | 0.2144                                                   | Moderate correlation with diabetes (0.2144), age (0.3374), hypertension (0.1477) | Important for diabetes risk.                                 |
| age                   | 0.2580                                                   | Moderate correlation with diabetes (0.2580), hypertension (0.2512), bmi (0.3374) | Important for diabetes risk.                                 |
| hypertension          | 0.1978                                                   | Moderate correlation with diabetes (0.1978), age (0.2512), heart_disease (0.1213) | Relevant but less predictive than hbA1c/blood_glucose.       |
| heart_disease         | 0.1717                                                   | Moderate correlation with diabetes (0.1717), age (0.2334), hypertension (0.1213) | Relevant but less predictive than top features.              |
| smoking_former        | 0.0979                                                   | Weak correlation with diabetes (0.0979), age (0.2165), bmi (0.1113) | Marginal importance.                                         |
| smoking_never         | 0.0273                                                   | Weak correlation with diabetes (0.0273), bmi (0.0869)          | Negligible predictive power.                                 |
| smoking_not current   | 0.0207                                                   | Weak correlation with diabetes (0.0207)                        | Negligible predictive power.                                 |
| gender_Male           | 0.0377                                                   | Anti-correlated with gender_Female (-0.9996), weak with diabetes (0.0377) | Redundant (gender_Female is perfectly anti-correlated).      |
| gender_Female         | -0.0376                                                  | Anti-correlated with gender_Male (-0.9996), weak with diabetes (-0.0376) | Redundant (use one gender feature or drop both).             |
| race:AfricanAmerican  | 0.0044                                                   | Negligible correlation with diabetes (0.0044)                  | No predictive power.                                         |
| race:Asian            | 0.0038                                                   | Negligible correlation with diabetes (0.0038)                  | No predictive power.                                         |
| race:Caucasian        | -0.0017                                                  | Negligible correlation with diabetes (-0.0017)                 | No predictive power.                                         |
| race:Hispanic         | -0.0013                                                  | Negligible correlation with diabetes (-0.0013)                 | No predictive power.                                         |
| race:Other            | -0.0051                                                  | Negligible correlation with diabetes (-0.0051)                 | No predictive power.                                         |
| smoking_current       | 0.0196                                                   | Negligible correlation with diabetes (0.0196)                  | No predictive power.                                         |
| smoking_ever          | 0.0241                                                   | Negligible correlation with diabetes (0.0241)                  | No predictive power.                                         |
| smoking_No Info       | -0.1189                                                  | Negligible correlation with diabetes (-0.1189)                 | No predictive power.                                         |
| year                  | -0.0034                                                  | Negligible correlation with diabetes (-0.0034)                 | No predictive power.                                         |
| location_encoded      | -0.0039                                                  | Negligible correlation with diabetes (-0.0039)                 | No predictive power.                                         |
| gender_Other          | -0.0041                                                  | Negligible correlation with diabetes (-0.0041)                 | No predictive power.                                         |

# Columns to Drop

### Race features (near-zero correlation)
    'race:AfricanAmerican', 'race:Asian', 'race:Caucasian', 'race:Hispanic', 'race:Other'
    
### Smoking features (very weak/no predictive power)
    'smoking_No Info', 'smoking_current', 'smoking_ever', 'smoking_former','smoking_never', 'smoking_not current',

### Gender features (redundant due to perfect anti-correlation)
    'gender_Female', 'gender_Male', 'gender_Other',

### Other non-predictive features
    'year', 'location_encoded'
"""

columns_to_drop = [
    'smoking_No Info', 'smoking_current', 'smoking_ever', 'smoking_former',
    'smoking_never', 'smoking_not current',
    'gender_Female', 'gender_Male', 'gender_Other',
    "year",
    "location_encoded",
    "race:AfricanAmerican","race:Asian","race:Caucasian","race:Hispanic","race:Other"
]

df = df.drop(columns=columns_to_drop, errors='ignore')

df.head()

df.isnull().sum()

df['diabetes'].value_counts()

for col in ["age","hypertension","heart_disease","bmi","hbA1c_level","blood_glucose_level"]:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Box Plot for {col}')
    plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='bmi', data=df)
plt.title('BMI Distribution by Diabetes Status')
plt.show()

df.shape

df['diabetes'].value_counts()

def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Filter out outliers
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

df1= remove_outliers_iqr(df, ["age","hypertension","heart_disease","bmi","hbA1c_level","blood_glucose_level"])

for col in ["age","hypertension","heart_disease","bmi","hbA1c_level","blood_glucose_level"]:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df1[col])
    plt.title(f'Box Plot for {col}')
    plt.show()

df1.shape

df1['diabetes'].value_counts()

df1['diabetes'].value_counts(normalize=True)

"""lost significant minority class samples (1):
From 8,500 → 2,851 → a loss of ~66%!

class imbalance worsened severely:
From ~91%/9% → ~96.5%/3.5%


Many outlier detection methods are class-agnostic — they don’t consider class label.

In healthcare, "outliers" in diabetic patients (e.g., very high blood glucose or hbA1c) might be true patterns, not noise.

Removing them harms model learning, especially for the class you're most interested in.
"""

df['diabetes'].value_counts()

# Age Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['age'], kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# BMI Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['bmi'], kde=True)
plt.title('BMI Distribution')
plt.xlabel('BMI')
plt.ylabel('Frequency')
plt.show()

# HbA1c Level Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['hbA1c_level'], kde=True)
plt.title('HbA1c Level Distribution')
plt.xlabel('HbA1c Level')
plt.ylabel('Frequency')
plt.show()

# Blood Glucose Level Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['blood_glucose_level'], kde=True)
plt.title('Blood Glucose Level Distribution')
plt.xlabel('Blood Glucose Level')
plt.ylabel('Frequency')
plt.show()

# Diabetes Class Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='diabetes', data=df)
plt.title('Class Distribution (Diabetes)')
plt.xlabel('Diabetes')
plt.ylabel('Count')
plt.show()

# Age vs Diabetes
plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='age', data=df)
plt.title('Age vs Diabetes')
plt.xlabel('Diabetes')
plt.ylabel('Age')
plt.show()

# BMI vs Diabetes
plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='bmi', data=df)
plt.title('BMI vs Diabetes')
plt.xlabel('Diabetes')
plt.ylabel('BMI')
plt.show()

# Blood Glucose Level vs Diabetes
plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='blood_glucose_level', data=df)
plt.title('Blood Glucose Level vs Diabetes')
plt.xlabel('Diabetes')
plt.ylabel('Blood Glucose Level')
plt.show()

# HbA1c Level vs Diabetes
plt.figure(figsize=(10, 6))
sns.boxplot(x='diabetes', y='hbA1c_level', data=df)
plt.title('HbA1c Level vs Diabetes')
plt.xlabel('Diabetes')
plt.ylabel('HbA1c Level')
plt.show()

# Hypertension vs Diabetes
plt.figure(figsize=(6, 4))
sns.countplot(x='hypertension', hue='diabetes', data=df)
plt.title('Hypertension vs Diabetes')
plt.xlabel('Hypertension')
plt.ylabel('Count')
plt.show()

# Heart Disease vs Diabetes
plt.figure(figsize=(6, 4))
sns.countplot(x='heart_disease', hue='diabetes', data=df)
plt.title('Heart Disease vs Diabetes')
plt.xlabel('Heart Disease')
plt.ylabel('Count')
plt.show()

# Pair Plot of all features
sns.pairplot(df, hue='diabetes')
plt.suptitle('Pair Plot of All Features', y=1.02)
plt.show()

# Correlation Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Facet Grid for Age Distribution by Diabetes Class
g = sns.FacetGrid(df, col="diabetes", height=5)
g.map(sns.histplot, "age", kde=True)
g.set_axis_labels('Age', 'Frequency')
plt.suptitle('Age Distribution by Diabetes Class', y=1.02)
plt.show()

# Box Plot for each numerical feature
df[['age', 'bmi', 'hbA1c_level', 'blood_glucose_level']].plot(kind='box', subplots=True, layout=(2, 2), figsize=(12, 8))
plt.suptitle('Box Plot for Each Numerical Feature')
plt.show()

# Count Plot for Diabetes Target Class
plt.figure(figsize=(6, 4))
sns.countplot(x='diabetes', data=df)
plt.title('Class Distribution (Diabetes)')
plt.xlabel('Diabetes')
plt.ylabel('Count')
plt.show()

X = df.drop('diabetes', axis=1)
y = df['diabetes']

X.head()

# Handling class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Class distribution after SMOTE
pd.Series(y_res).value_counts(normalize=True)

# Use stratify=y_res to maintain the same class ratio in train/test
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled[:5]

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    #'Support Vector Machine': SVC(probability=True, random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')
}

# Dictionary to store results
results = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': [],
    'AUC-ROC': []
}

# Train and evaluate each model
for name, model in models.items():
    # Train the model
    model.fit(X_train_scaled, y_train)

    # Make predictions
    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1]

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_prob)

    # Store results
    results['Model'].append(name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)
    results['F1-Score'].append(f1)
    results['AUC-ROC'].append(auc_roc)

    # Print confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # Plot ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_roc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

# Convert results to DataFrame
results_df = pd.DataFrame(results)
print(results_df)
results_df.to_csv("results.csv")

plt.figure(figsize=(12, 8))
results_df.plot(x='Model', y=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],
                kind='bar', figsize=(14, 8))
plt.title('Model Performance Comparison')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Feature Importance using Random Forest
rf = models['Random Forest']
importances = rf.feature_importances_
feature_names = X.columns

feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance (Random Forest)')
plt.tight_layout()
plt.show()

best_model = results_df.loc[results_df['AUC-ROC'].idxmax()]
print("\nBest Model:")
print(best_model)

fina_model=models['XGBoost']

#input_data = np.array([[32.0, 0, 0, 27.32, 5.0, 100]])
input_data = np.array([[18.0,0,0,23.76,4.8,160]])

fina_model.predict(input_data)